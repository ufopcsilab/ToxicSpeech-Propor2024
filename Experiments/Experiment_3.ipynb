{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv5jEi8EB5tN",
        "outputId": "fdd27662-3b31-4a0f-d6cc-6dec5035f4c4"
      },
      "outputs": [],
      "source": [
        "%cd PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKapvQ9uUGNQ"
      },
      "source": [
        "Prompt 3:\n",
        "\n",
        "\"Você está realizando análise de texto. O texto a seguir foi utilizado em um contexto de uma rede social. \"\n",
        "                \"Responda apenas 'sim' ou 'não' caso o texto abaixo contenha discurso de ódio, ofensa, agressividade ou outras formas de toxicidade. Caso o texto contenha palavrões, palavras de baixo calão ou palavras obsenas, classifique como texto ofensivo, portanto tóxico: \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeNHzyynhU_m",
        "outputId": "378c868a-6e2b-4639-91b9-eac86edd29af"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WvVY6CRa9rW",
        "outputId": "e667f05e-6f45-4f61-e8d6-714fdfa033ed"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Function to map classifications\n",
        "def mapear_classificacao(texto):\n",
        "    # Check if the text is already numerical\n",
        "    if isinstance(texto, (int, float)):\n",
        "        return texto\n",
        "\n",
        "    texto = unidecode(texto).lower()  # Remove accents and convert to lowercase\n",
        "\n",
        "    if re.search(r'\\bsim\\b', texto):\n",
        "        return 1.0\n",
        "    elif re.search(r'\\bnao\\b', texto):\n",
        "        return 0.0\n",
        "    else:  # For other non -corresponding variations or texts\n",
        "        return 1.0\n",
        "\n",
        "# Explicit choice of 4 files\n",
        "selected_files = [\n",
        "    'Maritaca_datasetName_zeroshot_prompt_2.csv',\n",
        "    'Maritaca_datasetName_few-shot_10_prompt2_v2.csv',\n",
        "    'Maritaca_datasetName_zeroshot_prompt_3.csv'\n",
        "]\n",
        "\n",
        "# Mapping the names of the model for a name of your choice\n",
        "model_name_mapping = {\n",
        "    'Maritaca_datasetName_zeroshot_prompt_2': 'MariTalk (Sabiá-65B) Zero-shot - Prompt 2',\n",
        "    'Maritaca_datasetName_few-shot_10_prompt2_v2': 'MariTalk (Sabiá-65B) Few-shot - Prompt 2',\n",
        "    'Maritaca_datasetName_zeroshot_prompt_3': 'MariTalk (Sabiá-65B) Zero-shot - Prompt 3'\n",
        "}\n",
        "\n",
        "path = 'results'\n",
        "\n",
        "# Load the first CSV\n",
        "first_file = os.path.join(path, selected_files[0])\n",
        "df = pd.read_csv(first_file)\n",
        "final_df = df[['text', 'Toxic']]\n",
        "\n",
        "# Add the Predictions column from the first file to the final dataframe\n",
        "first_column_name = model_name_mapping[os.path.basename(os.path.splitext(selected_files[0])[0])]\n",
        "df['predictions'] = df['predictions'].apply(mapear_classificacao)\n",
        "final_df.loc[:, first_column_name] = df['predictions']\n",
        "\n",
        "# Item on the other selected CSV files\n",
        "for file in selected_files[1:]:\n",
        "    file_path = os.path.join(path, file)\n",
        "    temp_df = pd.read_csv(file_path)\n",
        "\n",
        "    # mapearAColunaPredictions\n",
        "    temp_df['predictions'] = temp_df['predictions'].apply(mapear_classificacao)\n",
        "\n",
        "    # Rename the Predictions column according to the mapping\n",
        "    column_name = model_name_mapping[os.path.basename(os.path.splitext(file)[0])]  # Map the file name to the personalized name\n",
        "    final_df[column_name] = temp_df['predictions']\n",
        "\n",
        "#print(final_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQlBT-BcP8P-",
        "outputId": "21c16c98-9516-4f6b-f5b1-9168d1223fcc"
      },
      "outputs": [],
      "source": [
        "!pip install mlxtend\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uBhF3Ee_gnKX",
        "outputId": "79eec3b2-7489-4a98-ba42-75d13c475318"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Mapping the names of the model for a name of your choice\n",
        "model_name_mapping = {\n",
        "    'Maritaca_datasetName_zeroshot_prompt_2': 'MariTalk (Sabiá-65B) Zero-shot - Prompt 2',\n",
        "    'Maritaca_datasetName_few-shot_10_prompt2_v2': 'MariTalk (Sabiá-65B) Few-shot - Prompt 2',\n",
        "    'Maritaca_datasetName_zeroshot_prompt_3': 'MariTalk (Sabiá-65B) Zero-shot - Prompt 3'\n",
        "}\n",
        "\n",
        "# For each model, calculate the classification report and the confusion matrix\n",
        "for col in final_df.columns:\n",
        "    if col not in ['text', 'Toxic']:\n",
        "        # Obtain the personalized name of the model\n",
        "        modelo_nome = model_name_mapping.get(col, col)  # If you don't find the name mapped, use the original name\n",
        "        print(f\"Modelo: {modelo_nome}\")\n",
        "\n",
        "        # Fill Nans with a default value (for example, 0.0)\n",
        "        final_df = final_df.fillna(1.0)\n",
        "\n",
        "        # Classification report\n",
        "        print(classification_report(final_df['Toxic'], final_df[col]))\n",
        "\n",
        "        #Confusion matrix\n",
        "        cm = confusion_matrix(final_df['Toxic'], final_df[col])\n",
        "\n",
        "        #Using MLXTEND to plot the confusion matrix\n",
        "        fig, ax = plot_confusion_matrix(conf_mat=cm,\n",
        "                                        show_absolute=True,\n",
        "                                        show_normed=True,\n",
        "                                        colorbar=False,\n",
        "                                        figsize=(10,7),\n",
        "                                        cmap=\"Greys\")\n",
        "        ax.set_title(f'{modelo_nome}')\n",
        "        plt.xlabel('Predicted labels')\n",
        "        plt.ylabel('True labels')\n",
        "        plt.show()\n",
        "\n",
        "        # False positive and false negative rates\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        tpr = tp / (tp + fn)  # True positive rate (sensitivity)\n",
        "        fpr = fp / (fp + tn)  #False rate\n",
        "        fnr = fn / (fn + tp)  # False negative rate\n",
        "        tnr = tn / (tn + fp)  # True negative rate (specificity)\n",
        "\n",
        "        print(f\"Taxa de Falso Positivo (FPR): {fpr:.2f}\")\n",
        "        print(f\"Taxa de Falso Negativo (FNR): {fnr:.2f}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRa99hpBzPp-",
        "outputId": "2c95e8bb-b477-4b4c-deb0-51b0464c1034"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Mapping the names of the model for a name of your choice\n",
        "model_name_mapping = {\n",
        "    'Maritaca_datasetName_zeroshot_prompt_2': 'MariTalk (Sabiá-65B) Zero-shot - Prompt 2',\n",
        "    'Maritaca_datasetName_few-shot_10_prompt2_v2': 'MariTalk (Sabiá-65B) Few-shot - Prompt 2',\n",
        "    'Maritaca_datasetName_zeroshot_prompt_3': 'MariTalk (Sabiá-65B) Zero-shot - Prompt 3'\n",
        "}\n",
        "\n",
        "# List for storing metrics of each model\n",
        "data = []\n",
        "\n",
        "# For each model, calculate the classification report\n",
        "for col in final_df.columns:\n",
        "    if col not in ['text', 'Toxic']:\n",
        "        # Obtain the personalized name of the model\n",
        "        modelo_nome = model_name_mapping.get(col, col)  # If you don't find the name mapped, use the original name\n",
        "        print(f\"Modelo: {modelo_nome}\")\n",
        "\n",
        "        # Classification report\n",
        "        report = classification_report(final_df['Toxic'], final_df[col], output_dict=True)\n",
        "        precision_0, recall_0, f1_0 = report['0.0']['precision'], report['0.0']['recall'], report['0.0']['f1-score']\n",
        "        precision_1, recall_1, f1_1 = report['1.0']['precision'], report['1.0']['recall'], report['1.0']['f1-score']\n",
        "\n",
        "        # Add metrics to the list\n",
        "        data.append([modelo_nome, precision_0, recall_0, f1_0, precision_1, recall_1, f1_1])\n",
        "\n",
        "# Convert List to Dataframe\n",
        "df_metrics = pd.DataFrame(data, columns=['Modelo', 'Precision_NonToxic', 'Recall_NonToxic', 'F1_NonToxic', 'Precision_Toxic', 'Recall_Toxic', 'F1_Toxic'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "Eulr4H6v1r7M",
        "outputId": "4724bf86-0557-401a-dbd8-2f6581ddfdbc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to create radar/spider chart\n",
        "def plot_spider_chart(df, title):\n",
        "    # Number of variables\n",
        "    categories = list(df)[1:]\n",
        "    N = len(categories)\n",
        "\n",
        "    # Angles for each axis\n",
        "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
        "    angles += angles[:1]\n",
        "\n",
        "    # Initialize the chart\n",
        "    ax = plt.subplot(111, polar=True)\n",
        "\n",
        "    # First axis at the top\n",
        "    ax.set_theta_offset(np.pi / 2)\n",
        "    ax.set_theta_direction(-1)\n",
        "\n",
        "    # Labels for each axis\n",
        "    plt.xticks(angles[:-1], categories)\n",
        "\n",
        "    # Define the label for the Y axis\n",
        "    ax.set_rlabel_position(0)\n",
        "    plt.yticks([0.2, 0.4, 0.6, 0.8], [\"0.2\", \"0.4\", \"0.6\", \"0.8\"], color=\"grey\", size=7)\n",
        "    plt.ylim(0,1)\n",
        "\n",
        "    # Colors for each model\n",
        "    colors = ['b', 'r', 'y', 'g', 'c', 'm', 'k', 'orange']\n",
        "\n",
        "\n",
        "    # Plot metrics for each model\n",
        "    for index, row in df.iterrows():\n",
        "        values = row.drop('Modelo').values.flatten().tolist()\n",
        "        values += values[:1]\n",
        "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=row['Modelo'], color=colors[index % len(colors)])\n",
        "        ax.fill(angles, values, color=colors[index % len(colors)], alpha=0.1)\n",
        "\n",
        "    # Legend\n",
        "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
        "\n",
        "    # Title\n",
        "    plt.title(title, size=11, color='blue', y=1.1)\n",
        "\n",
        "# Plot radar/spider graph for all models on the same chart\n",
        "plt.figure(figsize=(10, 8))\n",
        "plot_spider_chart(df_metrics, \"\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOb53TtNCgVx"
      },
      "source": [
        "## Filter and display the first 20 instances where the zero-shot model made a mistake and the Few-Shot model hit.If there are less than 20 instances that meet this criterion, it will show them all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFc0ADjFCQQ5",
        "outputId": "2f47c0ff-ed38-48df-c79d-d1e2f1cfdd20"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Function to map classifications\n",
        "def mapear_classificacao(texto):\n",
        "    if texto in [0.0, 1.0]:\n",
        "        return texto\n",
        "\n",
        "    texto = unidecode(texto).lower()  # Remove accents and convert to lowercase\n",
        "\n",
        "    if re.search(r'\\bsim\\b', texto):\n",
        "        return 1.0\n",
        "    elif re.search(r'\\bnao\\b', texto):\n",
        "        return 0.0\n",
        "    else:  # For other non -corresponding variations or texts\n",
        "        return 0.0\n",
        "\n",
        "# CSVs\n",
        "path_zeroshot = 'Maritaca_datasetName_zeroshot_prompt_2.csv'\n",
        "path_fewshot = 'Maritaca_datasetName_few-shot_10_prompt2_v2.csv'\n",
        "\n",
        "df_zeroshot = pd.read_csv(path_zeroshot)\n",
        "df_fewshot = pd.read_csv(path_fewshot)\n",
        "\n",
        "# Map the prediction column\n",
        "df_zeroshot['predictions'] = df_zeroshot['predictions'].apply(mapear_classificacao)\n",
        "df_fewshot['predictions'] = df_fewshot['predictions'].apply(mapear_classificacao)\n",
        "\n",
        "# Identify instances where the zero-shot model made a mistake and the unce-shot got it right\n",
        "errors_zeroshot_correct_fewshot = df_zeroshot[(df_zeroshot['predictions'] != df_zeroshot['Toxic']) & (df_fewshot['predictions'] == df_zeroshot['Toxic'])]\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Show the first 20 instances\n",
        "print(errors_zeroshot_correct_fewshot[['text', 'Toxic', 'predictions']].head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-dgFvx3V0QAi",
        "outputId": "5a9911d5-0d4f-4182-98a0-c76aab244d41"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to create radar/spider chart\n",
        "def plot_spider_chart(df, title):\n",
        "    # Number of variables\n",
        "    categories = list(df)[1:]\n",
        "    N = len(categories)\n",
        "\n",
        "    # Angles for each axis\n",
        "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
        "    angles += angles[:1]\n",
        "\n",
        "    # Initialize the chart\n",
        "    ax = plt.subplot(111, polar=True)\n",
        "\n",
        "    # First axis at the top\n",
        "    ax.set_theta_offset(np.pi / 2)\n",
        "    ax.set_theta_direction(-1)\n",
        "\n",
        "    # Labels for each axis\n",
        "    plt.xticks(angles[:-1], categories)\n",
        "\n",
        "    # Define the label for the Y axis\n",
        "    ax.set_rlabel_position(0)\n",
        "    plt.yticks([0.2, 0.4, 0.6, 0.8], [\"0.2\", \"0.4\", \"0.6\", \"0.8\"], color=\"grey\", size=7)\n",
        "    plt.ylim(0,1)\n",
        "\n",
        "    # Plot metrics to the model\n",
        "    values = df.iloc[0].drop('Modelo').values.flatten().tolist()\n",
        "    values += values[:1]\n",
        "    ax.plot(angles, values, linewidth=2, linestyle='solid')\n",
        "    ax.fill(angles, values, 'b', alpha=0.1)\n",
        "\n",
        "    # Title\n",
        "    plt.title(title, size=11, color='blue', y=1.1)\n",
        "\n",
        "# Plot radar/spider graph for each model\n",
        "for index, row in df_metrics.iterrows():\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plot_spider_chart(pd.DataFrame(row).T, row['Modelo'])\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bof00X8WiW_c"
      },
      "source": [
        "## Instances in which Bertimbau hit exclusively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "3AG7Aj4xc9do",
        "outputId": "12cb704b-d1c6-4293-a33c-f24e189575f7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Configure so that the dataframe columns are not truncated when displayed\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Mask for instances in which Bertimbau_test_datasetName got it right\n",
        "bertimbau_acertos = final_df['Bertimbau_test_datasetName'] == final_df['labels']\n",
        "\n",
        "# Masks for instances in which other models made mistakes\n",
        "outros_modelos_erros = [final_df[col] != final_df['labels'] for col in final_df.columns if col not in ['text', 'labels', 'Bertimbau_test_datasetName']]\n",
        "\n",
        "#Combine all masks\n",
        "mascara_final = bertimbau_acertos\n",
        "for mascara in outros_modelos_erros:\n",
        "    mascara_final = mascara_final & mascara\n",
        "\n",
        "# Filter the dataframe using the final mask\n",
        "resultados_exclusivos_bertimbau = final_df[mascara_final]\n",
        "\n",
        "print(resultados_exclusivos_bertimbau)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWJd_ANcidwH"
      },
      "source": [
        "## Instances in which maritaca zero-shot hit exclusively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-885FpYXdtN7"
      },
      "outputs": [],
      "source": [
        "# Configure so that the dataframe columns are not truncated when displayed\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Mask for instances where maritaca_datasetName_zeroshot_prompt_2 hit\n",
        "maritaca_acertos = final_df['Maritaca_datasetName_zeroshot_prompt_2'] == final_df['labels']\n",
        "\n",
        "# Masks for instances in which other models made mistakes\n",
        "outros_modelos_erros = [final_df[col] != final_df['labels'] for col in final_df.columns if col not in ['text', 'labels', 'Maritaca_datasetName_zeroshot_prompt_2']]\n",
        "\n",
        "# Combine all masks\n",
        "mascara_final = maritaca_acertos\n",
        "for mascara in outros_modelos_erros:\n",
        "    mascara_final = mascara_final & mascara\n",
        "\n",
        "# Filter the dataframe using the final mask\n",
        "resultados_exclusivos_maritaca = final_df[mascara_final]\n",
        "\n",
        "print(resultados_exclusivos_maritaca)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v2CzT5il8HN"
      },
      "source": [
        "## Text length:\n",
        "\n",
        "We can create a new dataframe column to store the length of each text.Thus, analyze the error rate in relation to the length of the text for each model.\n",
        "\n",
        "## Text complexity:\n",
        "\n",
        "We can use the Flesch-Kincaid readability index to calculate the complexity of the text.The Flesch-Kincaid index measures the complexity of the text based on the total number of words, sentences and syllables.\n",
        "And so, analyze the error rate in relation to the complexity of the text for each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zza0u1PwmU5t"
      },
      "outputs": [],
      "source": [
        "!pip install textstat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEIAEW6Ol7lQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from textstat import flesch_kincaid_grade\n",
        "\n",
        "# Add text length column\n",
        "final_df['text_length'] = final_df['text'].apply(len)\n",
        "\n",
        "# Add text complexity column using Flesch-Kincaid\n",
        "final_df['text_complexity'] = final_df['text'].apply(flesch_kincaid_grade)\n",
        "\n",
        "# For each model, analyze the error rate in relation to the length and complexity of the text\n",
        "for col in final_df.columns:\n",
        "    if col not in ['text', 'labels', 'text_length', 'text_complexity']:\n",
        "        # Calculate errors\n",
        "        errors = final_df[final_df['labels'] != final_df[col]]\n",
        "\n",
        "        # Plot errors vs.Text length\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.scatter(errors['text_length'], errors['text_complexity'], alpha=0.5)\n",
        "        plt.title(f'Model errors {col} vs. Text length and complexity')\n",
        "        plt.xlabel('Text length')\n",
        "        plt.ylabel('Text complexity (Flesch-Kincaid)')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou6pQa1dnxE9"
      },
      "source": [
        "Text length: If most errors occur in shorter texts (0-100 characters), it may be that these texts are more ambiguous, containing short sentences, exclamations or colloquial language that models have difficulty interpreting.\n",
        "\n",
        "Text complexity: If errors are concentrated in low complexity texts (0-10 in the Flesch-Kincaid index), this may indicate that models have difficulty dealing with simple but ambiguous language, or with slang and colloquialism."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
